GNN-RAG³ 四阶段训练流程与关键实现详解

一、顶层入口与阶段编排
- 顶层脚本：`train_gnn_rag.py`
  - 类 `GNNRAGTrainer` 负责四阶段训练的配置与调度，调用 `python -m gnn.main` 运行模型训练。
  - 默认阶段：`warmup` → `joint1` → `pcst_distill` → `joint2`，阶段间通过 `load_checkpoint` 串联。
- 模块入口：`gnn/main.py`
  - 解析 `parsing.py` 定义的模型/优化/性能参数，初始化 `Trainer_KBQA` 并执行 `train` 或 `evaluate_single`。
- 核心训练类：`gnn/train_model.py::Trainer_KBQA`
  - 加载数据（`load_data` / `load_data_graft`）、构造模型（`ReaRev/NSM/GraftNet`）、设置优化器与调度器（Adam + ExponentialLR）、按 epoch 执行训练/评估、保存最佳 checkpoint（按 H1 / F1）。

二、数据加载与 batch 结构（标准路径）
- 文件：`gnn/dataset_load.py`
  - 类 `BasicDataLoader`/`SingleDataLoader`：从 JSON 加载问题与子图，构建局部实体映射与邻接矩阵，支持 LSTM 与多种 BERT 分词器。
  - `get_batch` 返回：
    - `local_entity`（候选实体局部ID，padding 为 `num_entity`）
    - `query_entities`（查询实体 one-hot / multi-hot）
    - `kb_adj_mat`（局部 KB 邻接：head/relation/tail/batch_id/fact_id/weights）
    - `query_text`（问题输入 token ids）
    - `seed_dist`（种子实体分布）
    - `answer_dist`（答案分布）
    - 有版本返回 `true_batch_id` 用于索引偏移纠正
  - 动态或预存构建局部邻接（受 `data_eff` 控制），支持 `fact_drop`。

三、模型与前向传播（ReaRev with RAG³）
- 文件：`gnn/models/ReaRev/rearev.py`
  - 关键参数：`use_appr`、`appr_alpha`、`cand_n`、`use_dde`、`hop_dim`、`dir_dim`、`use_pcst`、`pcst_lambda=[0.1,0.1,0.05]`、`gumbel_temp`、`mid_restart`、`train_hybrid_only`、`freeze_gnn`。
  - 初始化：`HybridRetriever`（APPR+语义融合）、`DDE`（方向-距离编码）、`PCSTLoss`（软正则/蒸馏）。可选冻结 `ReasonGNNLayer` 参数。
  - forward 流程：
    1) 预处理 batch 张量与 mask；初始化 reasoning 与指令（LSTM/BERT）
    2) warmup 特殊路径（当 `training && train_hybrid_only && use_appr`）：
       - 直接用 `HybridRetriever` 将 APPR 与语义相似度融合，得到 `pred_dist`，计算 QA 损失与指标（H1/F1），跳过 GNN。
    3) GNN 推理循环（`num_iter` × `num_gnn`）：
       - 可在中途（`j == num_gnn // 2`）重启分布为种子（`mid_restart`）
       - 当 `use_dde` 时，向 `ReasonGNNLayer` 传入几何偏置（距离/方向编码），否则常规消息传递
       - 记录分布历史并更新查询指令（`QueryReform`）
    4) 损失与输出：
       - `qa_loss = get_loss(pred_dist, answer_dist)` 基础 QA 损失（逐样本屏蔽无答案 case）
       - `total_loss = qa_loss + sum(pcst_lambda) * pcst_soft_regularizer(...)`（训练且启用 PCST 时）
       - 返回 `total_loss, pred, pred_dist, [H1,F1]`

四、RAG³增强组件细节
- 混合检索：`gnn/retrieval/hybrid_retriever.py`
  - `compute_appr(edge_index, num_nodes, seeds, edge_weights)`：APPR 分数（可带度惩罚）。
  - `fusion_net(z_q)->alpha_q∈(0,1)`：可学习融合权重；`fuse_scores` 支持 z-score 门控与非负截断。
  - `hybrid_rank`：融合排序，返回前 `N`（自适应或固定 `cand_n`）候选节点。
  - `build_candidate_subgraph`：向量化筛边，重映射节点 ID，返回子图及映射。
- DDE 几何偏置：`gnn/layers/dde.py`
  - `hop_embedding(dist)` 与 `dir_embedding(direction)`，通过 `geo_gate_net` 输出几何门控；支持缓存/向量化的边特征计算。
- PCST 正则层：`gnn/losses/pcst_loss.py`
  - `pcst_soft_regularizer(pi_edge, edge_cost, laplacian, temperature)`：成本（λ1·cost·π）+ 连通性近似（λ2·Var(π)）+ 稀疏性（λ3·||π||₁）。
  - `pcst_distill_bce(pred_edge_mask, target_edge_mask)`：蒸馏 BCE。
  - `compute_edge_costs(edge_index, distances, relation_costs)`：向量化组合距离/关系成本。
  - `forward(..., stage)`：`distill` 返回蒸馏损失；`joint` 返回软正则；`full` 同时返回二者。

五、训练循环与优化器
- `Trainer_KBQA.train`：
  - 每 epoch：对 `train` 批次计算 `loss.backward()`、梯度裁剪（`gradient_clip`），`optimizer.step()`，`lr_scheduler.step()`（如启用）；按 `eval_every` 评估，保存 `*_best.pt`（H1/F1）。
  - 评估：`evaluate`/`evaluate_single` 计算指标与日志；支持从 `checkpoint_dir` 加载 `load_experiment`。
- 优化器/调度：Adam（`lr`），可选 `ExponentialLR(decay_rate)`；标签平滑 `label_smooth`。

六、参数来源与默认值（`gnn/parsing.py`）
- 模型选择：`ReaRev | NSM | GraftNet`（通过子解析器）
- 关键训练超参：`num_epoch=100, batch_size=20, lr=5e-4, eval_every=2, gradient_clip=1.0, fact_drop=0` 等。
- RAG³ 专属：`--use_appr --appr_alpha 0.85 --cand_n 1200 --use_dde --hop_dim 16 --dir_dim 8 --use_pcst --pcst_lambda 0.1,0.1,0.05 --mid_restart`；warmup 控制：`--train_hybrid_only --freeze_gnn`。
- 多 GPU：`--use_multi_gpu --gpu_ids "0,1"`；显存限制：`--gpu_memory_limit_gb`。

七、四阶段默认配置（`train_gnn_rag.py::GNNRAGTrainer.train`）
- warmup：`max_steps=5000, lr=1e-3, batch_size=32, eval_every=500, freeze_gnn, train_hybrid_only`
- joint1：`max_steps=20000, lr=5e-4, batch_size=16, eval_every=1000, load_checkpoint=warmup_best.pt`
- pcst_distill：`max_steps=10000, lr=1e-4, batch_size=16, eval_every=1000, load_checkpoint=joint1_best.pt`
- joint2：`max_steps=20000, lr=2e-4, batch_size=16, eval_every=1000, load_checkpoint=pcst_distill_best.pt`
- 每阶段调用 `python -m gnn.main ReaRev ...`，自动设置 `--experiment_name gnn_rag3_<dataset>-<stage>` 并合并用户 config。

八、损失与反向传播要点
- QA 损失：逐样本 `get_loss`（来自基类），`calc_loss_label` 做 mask 并平均到 batch。
- PCST 软正则：在训练且启用 `use_pcst` 时引入；`total_loss = qa_loss + Σλ · pcst_soft_regularizer`（`Σλ`=`sum(pcst_lambda)`）。
- Warmup：仅训练融合与 QA 损失（混合检索路径），跳过 GNN。
- 反向传播：`total_loss.backward()`，梯度裁剪与优化器更新；冻结 GNN 时仅更新检索与上层参数。

九、脚本与命令样例
- Windows/PowerShell：
  - `python train_gnn_rag.py --config configs\webqsp_train_config_sbert.json --stages warmup,joint1,pcst_distill,joint2`
- Linux 脚本示例（见 `gnn/scripts/rearev_cwq.sh`）：
  - `python train_gnn_rag.py --config configs/webqsp_train_config_sbert_linux.json --use_optimization --enable_sync_optimization --enable_adaptive_subgraph --enable_memory_monitoring`

十、产物与报告
- Checkpoints：`gnn/checkpoint/pretrain/<experiment_name>/*_best.pt`（按阶段别名保存/加载）。
- 输出目录：`gnn/outputs/gnn_rag3_<dataset>/training.log`；最终评估报告由 `train_gnn_rag.py::final_evaluation` 与 `generate_report` 生成（如在配置中启用）。

注意事项
- 若使用 `--gpu_memory_limit_gb`，`gnn/main.py` 会尝试调用 `torch.cuda.set_per_process_memory_fraction`（CUDA 可用时）。
- `eval_every` 在顶层脚本中会按数量级缩放以适配 `gnn.main` 的 epoch 频率（`>=100` 时除以 100 后下限为 1）。
- PCST 的 `laplacian` 在当前实现为简化近似，真实图拉普拉斯可按子图显式构造。